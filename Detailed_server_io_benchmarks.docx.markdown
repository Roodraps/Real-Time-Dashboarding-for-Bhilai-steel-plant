> This document contains the detailed benchmarks, and stress testing for
> our I/O and InfluxDB, we wanted to see the peak of our performance
> that we can achieve on our current storage devices which are installed
> on the server (10.1.10.194) along with other currently installed
> hardware. As we are going to perform various queries also millions of
> records need to be inserted into our Time Series Database this
> benchmark is needed.
> 
> The current hardware specifications of the server are mentioned below,
> although the detailed hardware specifications will be attached in
> another file with this.
> 
> All mount points of the server:

![](media_assets/Detailed_server_io_benchmarks.docx/media/image1.png)

> We first do the default stress I/O test on both of our storage blocks
> (Samsung SSD 870 EVO 1TB and TOSHIBA MG04ACA200E). I have used ***fio
> (https://git.kernel.dk/cgit/fio/)*** tool for this. fio is a great
> tool if we want to do a special test case or to test a specific
> workload to find out the performance or to reproduce a bug.
> 
> I have defined some special test cases as per our needs below:

  - > IOPS (Input-Output Operations per Second) Test
    
      - > Random R test
    
      - > Custom R/W test file
    
      - > Random R/W test
    
      - > Sequential R test

  - > Throughput Performance Tests
    
      - > Random R test
    
      - > Custom R/W test file
    
      - > Random R/W test
    
      - > Sequential R test

  - > Latency Performance Test
    
      - > Random R latency test
    
      - > Random R/W latency test

> The above tests will be done for both of our storage devices,
> ***although this is a test not actual production server test so I’ll
> create a script which can perform the same tests to other server and
> the device in case of future use***. The reason for doing these test
> is to get approximate figures on how much I/O and CPU performance need
> to be delivered for our actual production server.
> 
> Hardware information of /dev/sda (Samsung SSD 870 EVO 1TB):

![](media_assets/Detailed_server_io_benchmarks.docx/media/image2.png)

> Hardware information pf /dev/sdb:

![](media_assets/Detailed_server_io_benchmarks.docx/media/image3.png)

> ioengine: It defines how the job issued I/O by the file. There are
> various ioengines available to use for the test, some are listed
> below:

  - > libaio: Linux native asynchronous I/O. Note that Linux may only
    > support queued behaviour with non-buffered I/O (set direct=1 or
    > buffered=0). This engine defines engine-specific options.

  - > solarisaio: Solaris native asynchronous I/O. Suitable for testing
    > on Solaris.

  - > posixaio - POSIX asynchronous I/O. For other UNIX-based operating
    > systems.

  - > windowsaio - Windows native asynchronous I/O in case testing is
    > done on Windows OS.

  - > nfs - I/O engine supporting asynchronous read and write operations
    > to NFS from userspace via libnfs. This is useful for achieving
    > higher concurrency and thus throughput than is possible via kernel
    > NFS.

  - > net: Transfer over the network to given host:port. Depending on
    > the protocol used, the hostname, port, listen and filename options
    > are used to specify what sort of connection to make, while the
    > protocol option determines which protocol will be used. This
    > engine defines engine-specific options.

  - > libhdfs: Read and write through Hadoop (HDFS). The filename option
    > is used to specify the host, and port of the hdfs name-node to
    > connect. This engine interprets offsets a little differently. In
    > HDFS, files once created cannot be modified so random writes are
    > not possible. To imitate this the libhdfs engine expects a bunch
    > of small files to be created over HDFS and will randomly pick a
    > file from them based on the offset generated by fio backend (see
    > the example job file to create such files, use rw=write option).
    > Please note, it may be necessary to set environment variables to
    > work with HDFS/libhdfs properly. Each job uses its own connection
    > to HDFS.

  - > xnvme: I/O engine using the xNVMe C API, for NVMe devices. The
    > xnvme engine provides flexibility to access GNU/Linux Kernel NVMe
    > driver via libaio, IOCTLs, io\_uring, the SPDK NVMe driver, or
    > your own custom NVMe driver. The xnvme engine includes engine
    > specific options. (See
    > [<span class="underline">https://xnvme.io</span>](https://xnvme.io/)).

> We will be testing libaio which is default Linux native async I/O
> engine as well as posixaio, and maybe libhdfs, and nfs. xnvme can also
> be tested in case we use an nvme hardware for our production use.

# Bypassing Software/OS level caching

> The purpose of the storage benchmarking is to test the underlying
> storage and not the memory or OS caching capabilities so, I have
> disabled it using **–direct=1**. If value is true (1), use
> non-buffered I/O. This is usually O\_DIRECT. Note that OpenBSD and ZFS
> on Solaris don’t support direct I/O. On Windows, the synchronous
> ioengines don’t support direct I/O. Default: false. There can be some
> places where we will keep this false (0).

# File System vs Raw Disk

> Fio has the ability to execute tests against both a file system and a
> raw physical disc. Depending on the use situation, both choices should
> be taken into account. It is preferable to test by building a test
> file on top of the file system if the production applications will use
> Linux file systems, such as ext4 or zfs, for example. Testing against
> a raw drive without a file system will be more realistic if the
> production programme uses raw disc devices, such as Oracle's ASM.
> 
> Simply pointing the **—filename** to the disc name, for instance,
> **—filename=/dev/sda**, will work to get around the filesystem. Make
> sure to verify that the disk name is correct, because after running
> such a test all the data will be lost on the device, so specifying a
> wrong disk can be destructive. You should double-check the disc name
> before conducting the test because if you choose the wrong disc, all
> of the data on the device will be lost.

# Fio (.fio) setup

> The Fio test can be executed either from a file containing all the
> required parameters or from a single line stating all the required
> parameters in the command line.
> 
> It may be useful to construct several distinct jobfiles and then just
> trigger the tests by providing those files if it is necessary to run
> numerous different tests against.
> 
> I have made various fio files for the tests mentioned above, there
> will be shell script which will execute those files on storage device
> and will save results in a file.
> 
> **Fio main arguments**

<table>
<thead>
<tr class="header">
<th><blockquote>
<p><strong>--name=str</strong></p>
</blockquote></th>
<th><blockquote>
<p>Fio will create a file with the specified name to run the test on it. If the full path is entered, the file will be created at that path, if only a short name is provided, the file will be created in the current working directory.</p>
</blockquote></th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td><blockquote>
<p><strong>--ioengine=str</strong></p>
</blockquote></td>
<td><blockquote>
<p>This argument defines how the job issues I/O to the test file. There is a large amount of ioengines supported by Fio and the whole list can be found in the Fio documentation <a href="https://fio.readthedocs.io/en/latest/fio_doc.html#i-o-engine"><span class="underline">here</span>.</a></p>
<p>The engines worth mentioning are:</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p><strong>--size=int</strong></p>
</blockquote></td>
<td><blockquote>
<p>The size of the file on which the Fio will run the benchmarking test.</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p><strong>--rw=str</strong></p>
</blockquote></td>
<td><blockquote>
<p>Specifies the type of I/O pattern. The most common ones are as follows:</p>
</blockquote>
<ul>
<li><p><em>read:</em> sequential reads</p></li>
<li><p><em>write:</em> sequential writes</p></li>
<li><p><em>randread:</em> random reads</p></li>
<li><p><em>randwrite:</em> random writes</p></li>
</ul>
<ul>
<li><blockquote>
<p><em>rw:</em> sequential mix of reads and writes</p>
</blockquote></li>
</ul>
<ul>
<li><blockquote>
<p><em>randrw:</em> random mix of reads and writes</p>
</blockquote></li>
</ul>
<blockquote>
<p>Fio defaults to 50/50 if mixed workload is specified (rw=randrw). If more specific read/write distribution is needed, it can be configured with <strong>--rwmixread=</strong>. For example, -- rwmixread=30 would mean that 30% of the I/O will be reads and 70% will be writes.</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p><strong>--bs=int</strong></p>
</blockquote></td>
<td><blockquote>
<p>Defines the block size that the test will be using for generating the I/O. The default value is 4k and if not specified, the test will be using the default value. It is recommended to always specify the block size, because the default 4k is not commonly used by the applications.</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p><strong>--direct=bool</strong></p>
</blockquote></td>
<td><blockquote>
<p>true=1 or false=0. If the value is set to 1 (using non-buffered I/O) is fairer for testing as the benchmark will send the I/O directly to the storage subsystem bypassing the OS file system cache. The recommended value is always 1.</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p><strong>--numjobs=int</strong></p>
</blockquote></td>
<td><blockquote>
<p>The number of threads spawned by the test. By default, each thread is reported separately.</p>
<p>To see the results for all threads as a whole, use --group_reporting.</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p><strong>--iodepth=int</strong></p>
</blockquote></td>
<td><blockquote>
<p>Number of I/O units to keep in flight against the file. That is the amount of outstanding I/O for each thread.</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p><strong>--runtime=int</strong></p>
</blockquote></td>
<td><blockquote>
<p>The amount of time the test will be running in seconds.</p>
</blockquote></td>
</tr>
<tr class="odd">
<td><blockquote>
<p><strong>--time_based</strong></p>
</blockquote></td>
<td><blockquote>
<p>If given, run for the specified runtime duration even if the files are completely read or written. The same workload will be repeated as many times as runtime allows.</p>
</blockquote></td>
</tr>
<tr class="even">
<td><blockquote>
<p><strong>--startdelay</strong></p>
</blockquote></td>
<td><blockquote>
<p>Adds a delay in seconds between the initial test file creation and the actual test. Using a 60 seconds delay is recommended to allow the write cache (oplog) to drain after the test file is created and before the actual test starts to avoid reading the data from the oplog and to allow the oplog to be empty for the fair test.</p>
</blockquote></td>
</tr>
</tbody>
</table>

> **The fio files and results will be attached with this document, and
> also any future tests will be uploaded to our gdrive.**
